# EvalGate v0.3.0 Documentation

**EvalGate v0.3.0** is an open-source tool that runs deterministic LLM/RAG evaluations as GitHub PR checks.

---


# Quick Start

Get started with **EvalGate in less than 10 minutes**.

EvalGate runs deterministic LLM/RAG evaluations as GitHub PR checks. It compares your generated outputs against fixtures, validates formatting, accuracy, latency/cost budgets, and can use LLMs as judges for complex criteria.

## Installation

EvalGate works with Python 3.10+ and requires no additional infrastructure.

### Initialize EvalGate in your project

```bash
# Initialize EvalGate in your project
uvx --from evalgate evalgate init

# This creates:
# - .github/evalgate.yml (configuration)
# - eval/fixtures/ (test data with expected outputs)
# - eval/schemas/ (JSON schemas for validation)
```

### Generate Your Model's Outputs

```bash
# Run your model/system to generate outputs for the fixtures
# (Replace with your actual prediction script)
python scripts/predict.py --in eval/fixtures --out .evalgate/outputs
```

### Run Evaluation

```bash
# Run the evaluation suite
uvx --from evalgate evalgate run --config .github/evalgate.yml

# View results summary
uvx --from evalgate evalgate report --summary --artifact .evalgate/results.json
```

## GitHub Actions Integration

Add this workflow to `.github/workflows/evalgate.yml`:

```yaml
name: EvalGate
on: [pull_request]

jobs:
  evalgate:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      checks: write
    steps:
      - uses: actions/checkout@v4
        with: { fetch-depth: 0 }

      # Generate your model outputs
      - name: Generate outputs
        run: python scripts/predict.py --in eval/fixtures --out .evalgate/outputs

      # Run EvalGate
      - uses: aotp-ventures/evalgate@main
        with:
          config: .github/evalgate.yml
          check_run: true
```

## What's Next?

- **[Configuration Reference](/docs/configuration)** - Learn about all available evaluators and options
- **[Evaluator Types](/docs/evaluators)** - Explore the 13+ built-in evaluator types
- **[GitHub Actions Integration](/docs/github-actions)** - Advanced CI/CD setup
- **[Examples](/docs/examples)** - Real-world usage examples


---


# Configuration Reference

EvalGate uses a YAML configuration file (typically `.github/evalgate.yml`) to define evaluations.

## Complete Example

```yaml
# Performance budgets
budgets:
  p95_latency_ms: 1200
  max_cost_usd_per_item: 0.03

# Input fixtures
fixtures:
  path: "eval/fixtures/**/*.json"

# Model outputs
outputs:
  path: ".evalgate/outputs/**/*.json"

# Evaluators to run
evaluators:
  - name: json_formatting
    type: json_schema
    schema_path: "eval/schemas/response.json"
    weight: 0.4
  
  - name: priority_accuracy
    type: category_match
    expected_field: "priority"
    weight: 0.4
  
  - name: latency_cost
    type: latency_cost
    weight: 0.2

# Gate configuration
gate:
  min_overall_score: 0.90
  allow_regression: false

# Reporting options
report:
  pr_comment: true
  artifact_path: ".evalgate/results.json"

# Baseline comparison
baseline:
  ref: "origin/main"

# Privacy settings
telemetry:
  mode: "local_only"
```

## Configuration Sections

### Budgets

Define performance and cost constraints for your model.

```yaml
budgets:
  p95_latency_ms: 1200           # 95th percentile latency limit
  max_cost_usd_per_item: 0.03    # Maximum cost per evaluation
  avg_latency_ms: 800            # Average latency limit (optional)
  max_latency_ms: 5000           # Hard latency limit (optional)
```

### Fixtures & Outputs

Specify where to find test data and model outputs.

```yaml
fixtures:
  path: "eval/fixtures/**/*.json"
  encoding: "utf-8"              # Optional, default: utf-8

outputs:
  path: ".evalgate/outputs/**/*.json"
  encoding: "utf-8"              # Optional, default: utf-8
```

### Evaluators

Configure the evaluation pipeline. Each evaluator has:
- `name`: Unique identifier
- `type`: Evaluator type (see [Evaluator Types](/docs/evaluators))
- `weight`: Relative importance (0.0-1.0)
- Type-specific configuration

```yaml
evaluators:
  # JSON Schema validation
  - name: schema_check
    type: json_schema
    schema_path: "eval/schemas/output.json"
    weight: 0.3
    strict: true                 # Optional, fail on any schema violation

  # Category matching
  - name: category_check
    type: category_match
    expected_field: "category"
    weight: 0.3
    case_sensitive: false        # Optional, default: false

  # LLM Judge
  - name: quality_judge
    type: llm_judge
    provider: openai             # openai, anthropic, azure, local
    model: gpt-4
    prompt_path: eval/prompts/judge.txt
    api_key_env_var: OPENAI_API_KEY
    weight: 0.4
    min_score: 0.75             # Optional, minimum acceptable score
    cache_responses: true        # Optional, default: true
```

### Gate Configuration

Control when evaluations should pass or fail.

```yaml
gate:
  min_overall_score: 0.90       # Minimum weighted average score
  allow_regression: false       # Allow scores to drop vs baseline
  max_violations: 0             # Maximum number of violations allowed
  required_evaluators:          # Optional, specific evaluators that must pass
    - json_formatting
    - priority_accuracy
```

### Reporting

Configure how results are reported and stored.

```yaml
report:
  pr_comment: true              # Post results as PR comment
  artifact_path: ".evalgate/results.json"
  format: "detailed"            # "summary" or "detailed"
  include_violations: true      # Include violation details
  max_comment_length: 65536     # Optional, truncate long comments
```

### Baseline Comparison

Compare current results against a baseline (usually main branch).

```yaml
baseline:
  ref: "origin/main"            # Git reference for baseline
  file_path: ".evalgate/results.json"  # Optional, custom baseline file
  auto_update: false            # Optional, auto-update baseline on main
```

### Telemetry & Privacy

Control data collection and sharing.

```yaml
telemetry:
  mode: "local_only"            # "local_only", "metrics_only", "full"
  endpoint: ""                  # Optional, custom telemetry endpoint
```

## Environment Variables

Some configuration can be overridden with environment variables:

- `EVALGATE_CONFIG` - Path to configuration file
- `EVALGATE_FIXTURES_PATH` - Override fixtures path
- `EVALGATE_OUTPUTS_PATH` - Override outputs path
- `EVALGATE_TELEMETRY_MODE` - Override telemetry mode

## Advanced Configuration

### Multiple Configuration Files

You can split configuration across multiple files:

```yaml
# .github/evalgate.yml
extends: 
  - eval/base-config.yml
  - eval/llm-judges.yml

# Override specific settings
gate:
  min_overall_score: 0.95
```

### Conditional Evaluators

Run different evaluators based on conditions:

```yaml
evaluators:
  - name: basic_validation
    type: json_schema
    schema_path: "eval/schemas/basic.json"
    weight: 0.5
    
  - name: advanced_validation
    type: llm_judge
    provider: openai
    model: gpt-4
    prompt_path: eval/prompts/advanced.txt
    weight: 0.5
    enabled_when: ${{ github.event_name == 'pull_request' }}
```

### Custom Scoring

Override default scoring behavior:

```yaml
scoring:
  method: "weighted_average"     # "weighted_average", "minimum", "product"
  normalize_scores: true         # Optional, normalize to 0-1 range
  penalty_weight: 0.1           # Optional, penalty for violations
```

## Validation

Validate your configuration:

```bash
uvx --from evalgate evalgate validate --config .github/evalgate.yml
```

## Next Steps

- **[Evaluator Types](/docs/evaluators)** - Learn about all available evaluators
- **[Examples](/docs/examples)** - See real-world configurations
- **[GitHub Actions](/docs/github-actions)** - CI/CD integration


---


# Evaluator Types

EvalGate includes 13+ built-in evaluator types to comprehensively test your AI systems.

## Core Evaluators

### JSON Schema Validation
Validates that outputs conform to expected JSON structure and data types.

```yaml
evaluators:
  - name: json_formatting
    type: json_schema
    schema_path: "eval/schemas/response.json"
    weight: 0.3
```

### Category Match
Checks if categorical fields (like labels, priorities, classifications) match expected values.

```yaml
evaluators:
  - name: priority_accuracy
    type: category_match
    expected_field: "priority"
    weight: 0.4
```

### Latency & Cost Budgets
Ensures your model meets performance and cost requirements.

```yaml
evaluators:
  - name: performance_check
    type: latency_cost
    weight: 0.3
```

## Advanced Evaluators

### LLM Judge
Use language models to evaluate complex criteria like quality, tone, and accuracy.

```yaml
evaluators:
  - name: content_quality
    type: llm_judge
    provider: openai
    model: gpt-4
    prompt_path: eval/prompts/quality_judge.txt
    weight: 0.4
    min_score: 0.75
```

### Conversation Flow
Validate multi-turn conversations and dialogue systems.

```yaml
evaluators:
  - name: conversation_check
    type: conversation_flow
    expected_final_field: content
    max_turns: 5
    weight: 0.3
```

### Tool Usage
Validate that AI agents use tools correctly and in the right sequence.

```yaml
evaluators:
  - name: tool_validation
    type: tool_usage
    expected_tool_calls: true
    weight: 0.2
```

## Text Analysis Evaluators

### Regex Match
Pattern-based text validation for specific formats or content requirements.

```yaml
evaluators:
  - name: format_check
    type: regex_match
    pattern: "^[A-Z]{2,3}-\\d{3,4}$"
    weight: 0.2
```

### ROUGE & BLEU Scores
Text similarity metrics for content generation tasks.

```yaml
evaluators:
  - name: text_similarity
    type: rouge_bleu
    metrics: ["rouge-l", "bleu"]
    weight: 0.3
```

### Embedding Similarity
Semantic similarity using sentence transformers.

```yaml
evaluators:
  - name: semantic_similarity
    type: embedding_similarity
    model: "all-MiniLM-L6-v2"
    threshold: 0.8
    weight: 0.4
```

## Utility Evaluators

### Required Fields
Ensure all necessary fields are present in outputs.

```yaml
evaluators:
  - name: field_presence
    type: required_fields
    required_fields: ["id", "status", "priority"]
    weight: 0.2
```

### Classification Metrics
Calculate precision, recall, and F1 scores for classification tasks.

```yaml
evaluators:
  - name: classification_eval
    type: classification_metrics
    label_field: "category"
    weight: 0.3
```

### Workflow DAG
Validate complex multi-step workflows and dependencies.

```yaml
evaluators:
  - name: workflow_validation
    type: workflow_dag
    dag_file: "eval/workflows/process.yaml"
    weight: 0.3
```

## Custom Evaluators

Create your own evaluators by extending the base evaluator class:

```python
from evalgate.evaluators import BaseEvaluator
from evalgate.plugins import registry

@registry.evaluator("my_custom")
class MyCustomEvaluator(BaseEvaluator):
    def evaluate(self, outputs, fixtures, **kwargs):
        score = 1.0  # your scoring logic here
        violations: list[str] = []
        return score, violations
```

Then use it in your configuration:

```yaml
evaluators:
  - name: custom_check
    type: my_custom
    weight: 0.5
```

## Next Steps

- **[Configuration Reference](/docs/configuration)** - Complete YAML configuration options
- **[Examples](/docs/examples)** - Real-world usage examples
- **[GitHub Actions](/docs/github-actions)** - CI/CD integration
