# .github/workflows/selftest.yml
name: EvalGate Selftest
on:
  push:
    branches: [ main ]
  pull_request:

jobs:
  selftest:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
      checks: write
    steps:
      - uses: actions/checkout@v4
        with: { fetch-depth: 0 }

      - uses: actions/setup-python@v5
        with: { python-version: '3.11' }

      # Check if OpenAI API key is available (graceful degradation if not set)
      - name: Check OpenAI API Key availability
        id: check_api_key
        run: |
          if [ -n "${{ secrets.OPENAI_API_KEY }}" ]; then
            echo "âœ… OPENAI_API_KEY is available - will test LLM judge"
            echo "has_api_key=true" >> $GITHUB_OUTPUT
          else
            echo "âš ï¸ OPENAI_API_KEY not set - will test basic evaluators only"
            echo "has_api_key=false" >> $GITHUB_OUTPUT
          fi

      # Generate outputs from fixtures
      - name: Generate outputs
        run: python scripts/predict.py --in eval/fixtures --out .evalgate/outputs

      # Create comprehensive test config (with LLM judge if API key available)
      - name: Create test config
        run: |
          if [ "${{ steps.check_api_key.outputs.has_api_key }}" = "true" ]; then
            echo "Creating config with LLM judge enabled"
            cat > .github/evalgate-selftest.yml << 'EOF'
          budgets: { p95_latency_ms: 1200, max_cost_usd_per_item: 0.03 }
          fixtures: { path: "eval/fixtures/**/*.json" }
          outputs: { path: ".evalgate/outputs/**/*.json" }
          evaluators:
            - { name: json_formatting, type: schema, schema_path: "eval/schemas/queue_item.json", weight: 0.25 }
            - { name: priority_accuracy, type: category, expected_field: "priority", weight: 0.25 }
            - { name: latency_cost, type: budgets, weight: 0.25 }
            - { name: content_quality, type: llm, provider: openai, model: "gpt-4o-mini", prompt_path: "eval/prompts/quality_judge.txt", api_key_env_var: "OPENAI_API_KEY", weight: 0.25 }
          gate: { min_overall_score: 0.75, allow_regression: false }
          report: { pr_comment: true, artifact_path: ".evalgate/results.json" }
          baseline: { ref: "origin/main" }
          telemetry: { mode: "local_only" }
          EOF
          else
            echo "Creating config with basic evaluators only"
            cp .github/evalgate.yml .github/evalgate-selftest.yml
          fi

      # Run EvalGate using the composite action
      - name: Run EvalGate Selftest
        id: evalgate
        continue-on-error: true
        uses: ./
        with:
          config: .github/evalgate-selftest.yml
          openai_api_key: ${{ secrets.OPENAI_API_KEY }}
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          azure_api_key: ${{ secrets.AZURE_API_KEY }}
          check_run: true

      # Show action outputs
      - name: Show EvalGate outputs
        if: always()
        run: |
          echo "Total score: ${{ steps.evalgate.outputs.total_score }}"
          echo "Passed: ${{ steps.evalgate.outputs.passed }}"
      
      # Generate report markdown
      - name: Generate report
        if: always()
        run: |
          uvx --from evalgate evalgate report --artifact ./.evalgate/results.json > evalgate-report.md
      
      # Upload results artifact for debugging
      - name: Upload EvalGate Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: evalgate-results
          path: .evalgate/results.json
          retention-days: 30
          overwrite: true
      
      # Post unified results as PR comment
      - name: Post EvalGate Selftest Results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            try {
              const {owner, repo} = context.repo;
              const pr = context.payload.pull_request.number;
              const marker = '<!-- EVALGATE_SELFTEST -->';
              const hasApiKey = '${{ steps.check_api_key.outputs.has_api_key }}' === 'true';
              const testType = hasApiKey ? 'All Evaluators (including LLM Judge)' : 'Basic Evaluators Only';
              const reportOutput = fs.readFileSync('evalgate-report.md', 'utf8');
              const runId = context.runId;
              const artifactLink = `https://github.com/${owner}/${repo}/actions/runs/${runId}`;
              const body = marker + `\n## ðŸ§ª EvalGate Selftest Results - ${testType}\n\n` + reportOutput + `\n\nðŸ“‹ **[Download detailed results](${artifactLink})** - Look for "evalgate-results" in the Artifacts section`;
              const {data: comments} = await github.rest.issues.listComments({owner, repo, issue_number: pr});
              const existing = comments.find(c => c.user.type === 'Bot' && c.body && c.body.includes(marker));
              if (existing) {
                await github.rest.issues.updateComment({owner, repo, comment_id: existing.id, body});
              } else {
                await github.rest.issues.createComment({owner, repo, issue_number: pr, body});
              }
            } catch (error) {
              console.log('Failed to post selftest results:', error.message);
            }

      # Enforce gate based on action output
      - name: Enforce EvalGate
        if: always()
        run: |
          if [ "${{ steps.evalgate.outputs.passed }}" != "true" ]; then
            echo "EvalGate gate failed"
            exit 1
          fi
